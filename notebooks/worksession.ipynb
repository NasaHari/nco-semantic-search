{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camelot-py\n",
      "  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from camelot-py) (8.2.1)\n",
      "Collecting chardet>=5.1.0 (from camelot-py)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.1 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from camelot-py) (2.3.2)\n",
      "Collecting openpyxl>=3.1.0 (from camelot-py)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from camelot-py) (20250506)\n",
      "Collecting pypdf<6.0,>=4.0 (from camelot-py)\n",
      "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pandas>=2.2.2 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from camelot-py) (2.3.1)\n",
      "Collecting tabulate>=0.9.0 (from camelot-py)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting opencv-python-headless>=4.7.0.68 (from camelot-py)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pypdfium2>=4 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from camelot-py) (4.30.0)\n",
      "Collecting numpy>=1.26.1 (from camelot-py)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile (from openpyxl>=3.1.0->camelot-py)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from pdfminer-six>=20240706->camelot-py) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from pdfminer-six>=20240706->camelot-py) (45.0.6)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (2.22)\n",
      "Downloading camelot_py-1.0.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tabulate, pypdf, numpy, et-xmlfile, chardet, openpyxl, opencv-python-headless, camelot-py\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "Successfully installed camelot-py-1.0.0 chardet-5.2.0 et-xmlfile-2.0.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 openpyxl-3.1.5 pypdf-5.9.0 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install camelot-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_file, start_page, end_page):\n",
    "    \"\"\"\n",
    "    Extracts tables from a PDF file using Camelot and saves them into an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the input PDF.\n",
    "        output_file (str): Path to save the extracted tables Excel file.\n",
    "        start_page (int): Starting page number.\n",
    "        end_page (int): Ending page number.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_tables = []\n",
    "    output_file=f\"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/{output_file}\"\n",
    "    # Loop through the specified page range with a progress bar\n",
    "    for page_num in tqdm(range(start_page, end_page + 1), desc=\"Extracting tables\"):\n",
    "        page = str(page_num)\n",
    "        try:\n",
    "            # First attempt: Lattice (for bordered tables)\n",
    "            tables = camelot.read_pdf(f\"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/{pdf_path}\", pages=page, flavor='lattice')\n",
    "\n",
    "            # If no tables found, try Stream (for whitespace-separated tables)\n",
    "            if tables.n == 0:\n",
    "                tables = camelot.read_pdf(pdf_path, pages=page, flavor='stream')\n",
    "\n",
    "            if tables.n == 0:\n",
    "                print(f\"[Page {page}] ❌ No tables found.\")\n",
    "            else:\n",
    "                for table in tables:\n",
    "                    df = table.df\n",
    "                    df.columns = df.iloc[0]    # Use first row as header\n",
    "                    df = df[1:]      \n",
    "                    df.insert(0, \"Page\", page)  # Add a 'Page' column\n",
    "                    all_tables.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Page {page}] ⚠️ Error: {e}\")\n",
    "\n",
    "    # Combine and save tables if any were extracted\n",
    "    if all_tables:\n",
    "        combined_df = pd.concat(all_tables, ignore_index=True)\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n✅ All tables saved to: {output_file}\")\n",
    "        print(\"\\n📌 Last few rows of the combined extracted table:\")\n",
    "        print(combined_df.tail())\n",
    "    else:\n",
    "        print(\"\\n❌ No tables extracted from any page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting tables: 100%|██████████| 133/133 [01:41<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Page 384] ⚠️ Error: [Errno 2] No such file or directory: 'nco_vol1.pdf'\n",
      "\n",
      "✅ All tables saved to: /home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_vol1_alp_idx.csv\n",
      "\n",
      "📌 Last few rows of the combined extracted table:\n",
      "0    Page                               Occupational \\nTitle NCO \\n2015  \\\n",
      "3538  382  Working \\nProprietor, \\nStorage and \\nWarehousing  1120.2700   \n",
      "3539  382                  Working \\nProprietor, \\nTransport  1120.2300   \n",
      "3540  382               Working \\nProprietor, Water \\nSupply  1120.0300   \n",
      "3541  382  Working \\nProprietor, Well \\nDrilling/Propriet...  1120.1100   \n",
      "3542  383                                 Zoologist, General  2131.0900   \n",
      "\n",
      "0    NCO \\n2004  \n",
      "3538    1216.10  \n",
      "3539    1215.10  \n",
      "3540    1211.30  \n",
      "3541    1213.30  \n",
      "3542    2211.60  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_tables_from_pdf(\n",
    "    pdf_path=\"nco_vol1.pdf\",\n",
    "    output_file=\"nco_vol1_alp_idx.csv\",\n",
    "    start_page=252,\n",
    "    end_page=384\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ No repeated headers found.\n",
      "📁 Cleaned Excel saved to: /home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_vol1_alp_idx.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_vol1_alp_idx.csv\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_csv(excel_path)\n",
    "\n",
    "# Clean up columns with line breaks or extra spaces\n",
    "col1 = df.columns[1]\n",
    "col2 = df.columns[2]\n",
    "col3 = df.columns[3]\n",
    "\n",
    "# Normalize line breaks and whitespace in those 3 columns\n",
    "df[\"_col1_clean\"] = df[col1].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df[\"_col2_clean\"] = df[col2].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df[\"_col3_clean\"] = df[col3].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# Find duplicate header rows\n",
    "mask = (\n",
    "    (df[\"_col1_clean\"] == \"Occupational Title\") &\n",
    "    (df[\"_col2_clean\"] == \"NCO 2015\") &\n",
    "    (df[\"_col3_clean\"] == \"NCO 2004\")\n",
    ")\n",
    "\n",
    "header_indices = df[mask].index\n",
    "\n",
    "# Drop all but the first duplicate header row\n",
    "if len(header_indices) > 1:\n",
    "    df_cleaned = df.drop(header_indices[1:])\n",
    "    print(f\"✅ Removed {len(header_indices)-1} repeated header rows.\")\n",
    "else:\n",
    "    df_cleaned = df\n",
    "    print(\"ℹ️ No repeated headers found.\")\n",
    "\n",
    "# Drop helper columns\n",
    "df_cleaned = df_cleaned.drop(columns=[\"_col1_clean\", \"_col2_clean\", \"_col3_clean\"])\n",
    "\n",
    "# 🔻 Drop the first column (by index)\n",
    "df_cleaned = df_cleaned.iloc[:, 1:]\n",
    "\n",
    "# Save back to Excel\n",
    "df_cleaned.to_csv(excel_path, index=False)\n",
    "print(f\"📁 Cleaned Excel saved to: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved to:\n",
      "/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Set your PDF path\n",
    "pdf_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2a.pdf\"\n",
    "output_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test.txt\"\n",
    "\n",
    "# Define page range (note: page indices are 0-based)\n",
    "start_page = 13 - 1  # page 13\n",
    "end_page = 14-1    # page 75\n",
    "\n",
    "# Extract text from specified pages\n",
    "extracted_text = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:]\n",
    "    for i in range(start_page, end_page + 1):\n",
    "        page = pdf.pages[i]\n",
    "\n",
    "        # Page dimensions\n",
    "        width = page.width\n",
    "        height = page.height\n",
    "\n",
    "        # Slightly offset from exact center to avoid text cut-off\n",
    "        split_x = width * 0.52\n",
    "\n",
    "        # Crop left and right columns\n",
    "        left_col = page.crop((0, 0, split_x, height))\n",
    "        right_col = page.crop((split_x, 0, width, height))\n",
    "\n",
    "        # Extract text from each column\n",
    "        left_text = left_col.extract_text()\n",
    "        right_text = right_col.extract_text()\n",
    "\n",
    "        # Combine left then right column text for that page\n",
    "        combined_text = (left_text or \"\") + \"\\n\" + (right_text or \"\")\n",
    "        extracted_text.append(f\"\\n{combined_text}\")\n",
    "\n",
    "# # Save to text file\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\\n\".join(extracted_text))\n",
    "\n",
    "# print(f\"✅ Extracted column-wise text saved to: {output_path}\")\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_vol2a_p13_to_p75.txt\"\n",
    "output_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test.txt\"\n",
    "\n",
    "# Set of exact lines to remove\n",
    "EXACT_LINES_TO_REMOVE = {\n",
    "    \"National Classification of Occupations – 2015\",\n",
    "    \"VOLUME\",\n",
    "    \"VOLUME II A\",\n",
    "    \"VOLUME II B\"\n",
    "}\n",
    "\n",
    "# Regex patterns\n",
    "division_pattern = re.compile(r\"^Division\\s+\\d+$\")\n",
    "eii_pattern = re.compile(r\"^E\\s+II\\s+A\\s+\\d+$\")\n",
    "\n",
    "# Clean storage\n",
    "cleaned_lines = []\n",
    "skip_next_if_division = False\n",
    "\n",
    "# Process lines\n",
    "for line in extracted_text:\n",
    "    stripped = line.strip()\n",
    "\n",
    "    if not stripped:\n",
    "        continue\n",
    "\n",
    "    # Remove exact matches\n",
    "    if stripped in EXACT_LINES_TO_REMOVE:\n",
    "        skip_next_if_division = True\n",
    "        continue\n",
    "\n",
    "    # Remove Division line if flagged\n",
    "    if skip_next_if_division and division_pattern.match(stripped):\n",
    "        skip_next_if_division = False\n",
    "        continue\n",
    "\n",
    "    # Remove E II A n lines\n",
    "    if eii_pattern.match(stripped):\n",
    "        continue\n",
    "\n",
    "    # Reset flag if not matched\n",
    "    skip_next_if_division = False\n",
    "\n",
    "    # Keep the line\n",
    "    cleaned_lines.append(stripped)\n",
    "\n",
    "# Write cleaned output\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(cleaned_lines))\n",
    "\n",
    "print(f\"✅ Cleaned file saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LEFT COLUMN ===\n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] National Classification of Occupations – 2015   \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (1.0,)] Division \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (1.0,)] 1 \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (0.31, 0.506, 0.741)] Managers \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Managers plan, direct, co-ordinate and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] evaluate the overall activities of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] enterprises, governments and other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organizations, or of organizational units \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] within them, and formulate and review \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] their policies, laws, rules and regulations. \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Tasks performed by managers usually \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] include: formulating and advising on the \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] policy, budgets, laws and regulations of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] enterprises, governments and other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organizational units; establishing \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] objectives and standards and formulating \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] and evaluating programmes and policies \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] and procedures for their implementation; \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] ensuring appropriate systems and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] procedures are developed and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] implemented to provide budgetary \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] control; authorising material, human and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] financial resources to implement policies \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] and programmes. Monitoring and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] evaluating performance of the \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organization or enterprise and of its staff; \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] selecting, or approving the selection of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] staff; ensuring compliance with health and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] safety requirements; planning and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] directing daily operations; representing \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] and negotiating on behalf of the \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] government, enterprise or organizational \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] unit managed in meetings and other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] forums. \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Occupations in this Division are classified \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] into the following Sub Divisions: \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 11Chief Executives, Senior Officials \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] and Legislators \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 12Administrative and Commercial \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Managers \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] VOLUME\n",
      "\n",
      "=== RIGHT COLUMN ===\n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Division 1 \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 13Production and Specialized \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Services Managers \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 14Hospitality, Retail and Other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Services Managers \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (0.31, 0.506, 0.741)] Sub Division  11 \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (0.31, 0.506, 0.741)] Chief Executives, Senior Officials \n",
      "[Font: ABCDEE+Calibri,Bold | Color: (0.31, 0.506, 0.741)] and Legislators \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Chief Executives, Senior Officials and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Legislators formulate and review the \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] policies and plan, direct, co-ordinate and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] evaluate the overall activities of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] enterprises, governments and other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organizations with the support of other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] managers. \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Tasks performed usually include: presiding \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] over or participating in the proceedings of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] legislative bodies, board of directors and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] committees; formulating and advising on \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] the policy budgets, laws and regulations \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] of enterprises, governments and other \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organizations; establishing objectives for \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] enterprises, government departments or \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] agencies and other organizations; \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] formulating or approving and evaluating \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] programmes and policies and procedures \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] for their implementation; ensuring \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] appropriate systems and procedures are \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] developed and implemented to provide \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] budgetary control; authorising material, \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] human and financial resources to \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] implement policies and programmes; \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] monitoring and evaluating performance of \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] the organization or enterprise; selecting, \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] or approving the selection of senior staff; \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] performing ceremonial duties and \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] representing the enterprise, government, \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] organization or community at official \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] occasions and in meetings, negotiations, \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] conventions and public hearings. \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Occupations in this Sub Division are \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] classified into the following Groups: \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] Legislators and Senior Officials\n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 111 \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] 1 \n",
      "[Font: ABCDEE+Calibri | Color: (0.0,)] E II A \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from collections import defaultdict\n",
    "\n",
    "pdf_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2a.pdf\"\n",
    "start_page = 13 - 1  # zero-based index\n",
    "\n",
    "def color_to_rgb_string(color):\n",
    "    if isinstance(color, list) and len(color) == 3:\n",
    "        return f\"RGB({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)})\"\n",
    "    return str(color)\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    page = pdf.pages[start_page]\n",
    "    width = page.width\n",
    "    height = page.height\n",
    "    split_x = width * 0.52\n",
    "\n",
    "    # Process both left and right columns\n",
    "    for label, col in [(\"LEFT\", page.crop((0, 0, split_x, height))), (\"RIGHT\", page.crop((split_x, 0, width, height)))]:\n",
    "        print(f\"\\n=== {label} COLUMN ===\")\n",
    "\n",
    "        lines = defaultdict(list)\n",
    "\n",
    "        # Group characters into lines by y-position\n",
    "        for char in col.chars:\n",
    "            y0 = round(char['top'], 1)  # small rounding to group close y-values\n",
    "            lines[y0].append(char)\n",
    "\n",
    "        # Sort lines by y-position\n",
    "        for y0 in sorted(lines.keys()):\n",
    "            chars = sorted(lines[y0], key=lambda c: c['x0'])  # left to right\n",
    "            line_text = \"\".join(c['text'] for c in chars)\n",
    "\n",
    "            if not line_text.strip():\n",
    "                continue  # skip empty lines\n",
    "\n",
    "            # Use the first character's font and color\n",
    "            font = chars[0].get(\"fontname\", \"N/A\")\n",
    "            color = color_to_rgb_string(chars[0].get(\"non_stroking_color\", \"N/A\"))\n",
    "\n",
    "            print(f\"[Font: {font} | Color: {color}] {line_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 573/573 [00:37<00:00, 15.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted Title → Description pairs saved to:\n",
      "/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test_quali.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# === File Paths ===\n",
    "pdf_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2a.pdf\"\n",
    "output_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test_quali.txt\"\n",
    "\n",
    "# === Page Range ===\n",
    "start_page = 13 - 1\n",
    "end_page = 585 - 1  # Inclusive\n",
    "\n",
    "# === Font/Color Thresholds ===\n",
    "TITLE_FONT_KEYWORD = \"Bold\"\n",
    "TITLE_COLOR_RGB = (0.31, 0.506, 0.741)\n",
    "\n",
    "# === Junk Filters ===\n",
    "EXACT_LINES_TO_REMOVE = {\n",
    "    \"National Classification of Occupations – 2015\",\n",
    "    \"VOLUME\",\n",
    "    \"VOLUME II A\",\n",
    "    \"VOLUME II B\"\n",
    "}\n",
    "division_pattern = re.compile(r\"^Division\\s+\\d+$\")\n",
    "eii_pattern = re.compile(r\"^E\\s+II\\s+A\\s+\\d+$\")\n",
    "def remove_eii_patterns(text):\n",
    "    # Matches \"1 E II A\", \"2 E II A\", ..., etc., and removes that pattern\n",
    "    return re.sub(r\"\\b\\d+\\s+E\\s+II\\s+A\\b\", \"\", text)\n",
    "\n",
    "\n",
    "# === Utilities ===\n",
    "def is_same_color(c1, c2, tolerance=0.02):\n",
    "    return all(abs(a - b) <= tolerance for a, b in zip(c1, c2))\n",
    "\n",
    "\n",
    "def is_black(color):\n",
    "    if isinstance(color, (int, float)):\n",
    "        return color == 0.0\n",
    "    return all(c == 0.0 for c in color)\n",
    "def is_white(color, tolerance=0.01):\n",
    "    if isinstance(color, (int, float)):\n",
    "        return abs(color - 1.0) <= tolerance\n",
    "    return all(abs(c - 1.0) <= tolerance for c in color)\n",
    "\n",
    "def is_junk_line(text):\n",
    "    stripped = text.strip()\n",
    "    if not stripped:\n",
    "        return False\n",
    "    if stripped in EXACT_LINES_TO_REMOVE:\n",
    "        return True\n",
    "    if division_pattern.match(stripped):\n",
    "        return True\n",
    "    if eii_pattern.match(stripped):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def group_lines_by_y(chars):\n",
    "    lines = defaultdict(list)\n",
    "    for char in chars:\n",
    "        y = round(char['top'], 1)\n",
    "        lines[y].append(char)\n",
    "    return lines\n",
    "\n",
    "\n",
    "# === Main Extraction ===\n",
    "title_desc_blocks = []\n",
    "current_title = \"\"\n",
    "current_desc = []\n",
    "collecting_title = False\n",
    "from tqdm import tqdm\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_num in tqdm(range(start_page, end_page + 1), desc=\"Processing pages\"):\n",
    "        page = pdf.pages[page_num]\n",
    "        width, height = page.width, page.height\n",
    "        split_x = width * 0.52\n",
    "\n",
    "        for col in [page.crop((0, 0, split_x, height)), page.crop((split_x, 0, width, height))]:\n",
    "            chars = col.chars\n",
    "            lines = group_lines_by_y(chars)\n",
    "\n",
    "            for y0 in sorted(lines.keys()):\n",
    "                line_chars = sorted(lines[y0], key=lambda c: c['x0'])\n",
    "                line_text = \"\".join(c['text'] for c in line_chars).strip()\n",
    "                line_text = remove_eii_patterns(line_text).strip()\n",
    "\n",
    "                if not line_text:\n",
    "                    continue\n",
    "                if is_junk_line(line_text):\n",
    "                    continue\n",
    "\n",
    "                font = line_chars[0].get(\"fontname\", \"\")\n",
    "                color = line_chars[0].get(\"non_stroking_color\", [0, 0, 0])\n",
    "\n",
    "                is_title = TITLE_FONT_KEYWORD in font and (\n",
    "                    is_same_color(color, TITLE_COLOR_RGB) or is_black(color) or is_white(color)\n",
    "                )\n",
    "\n",
    "                if is_title:\n",
    "                    if collecting_title:\n",
    "                        current_title += \" \" + line_text\n",
    "                    else:\n",
    "                        if current_title and current_desc:\n",
    "                            title_desc_blocks.append((current_title.strip(), \" \".join(current_desc)))\n",
    "                        current_title = line_text\n",
    "                        current_desc = []\n",
    "                        collecting_title = True\n",
    "                else:\n",
    "                    current_desc.append(line_text)\n",
    "                    collecting_title = False\n",
    "\n",
    "# === Final Block Save ===\n",
    "if current_title and current_desc:\n",
    "    title_desc_blocks.append((current_title.strip(), \" \".join(current_desc)))\n",
    "\n",
    "# === Write Output ===\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for title, desc in title_desc_blocks:\n",
    "        f.write(f\"{title}\\n{desc}\\n\\n\")\n",
    "\n",
    "print(f\"✅ Extracted Title → Description pairs saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing /home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2a.pdf: 100%|██████████| 573/573 [00:37<00:00, 15.39it/s]\n",
      "Processing /home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2b.pdf: 100%|██████████| 567/567 [00:35<00:00, 15.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted Title → Description pairs saved to:\n",
      "/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test_quali.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Input files and page ranges ===\n",
    "pdf_ranges = [\n",
    "    (\"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2a.pdf\", 13 - 1, 585 - 1),\n",
    "    (\"/home/harikrishnan/Statathon/nco-semantic-search/data/raw/nco_vol2b.pdf\", 13 - 1, 579 - 1)\n",
    "]\n",
    "\n",
    "output_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test_quali.txt\"\n",
    "\n",
    "# === Font/Color Thresholds ===\n",
    "TITLE_FONT_KEYWORD = \"Bold\"\n",
    "TITLE_COLOR_RGB = (0.31, 0.506, 0.741)\n",
    "\n",
    "# === Junk Filters ===\n",
    "EXACT_LINES_TO_REMOVE = {\n",
    "    \"National Classification of Occupations – 2015\",\n",
    "    \"VOLUME\",\n",
    "    \"VOLUME II A\",\n",
    "    \"VOLUME II B\"\n",
    "}\n",
    "division_pattern = re.compile(r\"^Division\\s+\\d+$\")\n",
    "eii_pattern = re.compile(r\"^E\\s+II\\s+A\\s+\\d+$\")\n",
    "\n",
    "def remove_eii_patterns(text):\n",
    "    return re.sub(r\"\\b\\d+\\s+E\\s+II\\s+A\\b\", \"\", text)\n",
    "\n",
    "# === Utilities ===\n",
    "def is_same_color(c1, c2, tolerance=0.02):\n",
    "    return all(abs(a - b) <= tolerance for a, b in zip(c1, c2))\n",
    "\n",
    "def is_black(color):\n",
    "    if isinstance(color, (int, float)):\n",
    "        return color == 0.0\n",
    "    return all(c == 0.0 for c in color)\n",
    "\n",
    "def is_white(color, tolerance=0.01):\n",
    "    if isinstance(color, (int, float)):\n",
    "        return abs(color - 1.0) <= tolerance\n",
    "    return all(abs(c - 1.0) <= tolerance for c in color)\n",
    "\n",
    "def is_junk_line(text):\n",
    "    stripped = text.strip()\n",
    "    if not stripped:\n",
    "        return False\n",
    "    if stripped in EXACT_LINES_TO_REMOVE:\n",
    "        return True\n",
    "    if division_pattern.match(stripped):\n",
    "        return True\n",
    "    if eii_pattern.match(stripped):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def group_lines_by_y(chars):\n",
    "    lines = defaultdict(list)\n",
    "    for char in chars:\n",
    "        y = round(char['top'], 1)\n",
    "        lines[y].append(char)\n",
    "    return lines\n",
    "\n",
    "# === Main Extraction ===\n",
    "title_desc_blocks = []\n",
    "current_title = \"\"\n",
    "current_desc = []\n",
    "collecting_title = False\n",
    "\n",
    "for pdf_path, start_page, end_page in pdf_ranges:\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in tqdm(range(start_page, end_page + 1), desc=f\"Processing {pdf_path}\"):\n",
    "            page = pdf.pages[page_num]\n",
    "            width, height = page.width, page.height\n",
    "            split_x = width * 0.52\n",
    "\n",
    "            for col in [page.crop((0, 0, split_x, height)),\n",
    "                        page.crop((split_x, 0, width, height))]:\n",
    "                chars = col.chars\n",
    "                lines = group_lines_by_y(chars)\n",
    "\n",
    "                for y0 in sorted(lines.keys()):\n",
    "                    line_chars = sorted(lines[y0], key=lambda c: c['x0'])\n",
    "                    line_text = \"\".join(c['text'] for c in line_chars).strip()\n",
    "                    line_text = remove_eii_patterns(line_text).strip()\n",
    "\n",
    "                    if not line_text or is_junk_line(line_text):\n",
    "                        continue\n",
    "\n",
    "                    font = line_chars[0].get(\"fontname\", \"\")\n",
    "                    color = line_chars[0].get(\"non_stroking_color\", [0, 0, 0])\n",
    "\n",
    "                    is_title = TITLE_FONT_KEYWORD in font and (\n",
    "                        is_same_color(color, TITLE_COLOR_RGB) or\n",
    "                        is_black(color) or is_white(color)\n",
    "                    )\n",
    "\n",
    "                    if is_title:\n",
    "                        if collecting_title:\n",
    "                            current_title += \" \" + line_text\n",
    "                        else:\n",
    "                            if current_title and current_desc:\n",
    "                                title_desc_blocks.append((current_title.strip(), \" \".join(current_desc)))\n",
    "                            current_title = line_text\n",
    "                            current_desc = []\n",
    "                            collecting_title = True\n",
    "                    else:\n",
    "                        current_desc.append(line_text)\n",
    "                        collecting_title = False\n",
    "\n",
    "# Add last collected block\n",
    "if current_title and current_desc:\n",
    "    title_desc_blocks.append((current_title.strip(), \" \".join(current_desc)))\n",
    "\n",
    "# === Save Output ===\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for title, desc in title_desc_blocks:\n",
    "        f.write(f\"{title}\\n{desc}\\n\\n\")\n",
    "\n",
    "print(f\"✅ Extracted Title → Description pairs saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>QP_NOS Reference</th>\n",
       "      <th>QP_NOS Name</th>\n",
       "      <th>NSQF_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1120.3402</td>\n",
       "      <td>MES/Q0207</td>\n",
       "      <td>Account Director (Advertising Agency)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1120.3500</td>\n",
       "      <td>MES/Q0201</td>\n",
       "      <td>Sales Director (Media Org)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1213.0102</td>\n",
       "      <td>ASC/Q6305</td>\n",
       "      <td>Quality Assurance Standards In Charge-Level 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1213.0202</td>\n",
       "      <td>ASC/Q0603</td>\n",
       "      <td>Area Service Manager</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1213.9900</td>\n",
       "      <td>ASC/Q0602</td>\n",
       "      <td>Territory Service Manager</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>9333.9900</td>\n",
       "      <td>LSC/Q1110</td>\n",
       "      <td>Loader/Unloader</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>9611.9900</td>\n",
       "      <td>TEL/Q2400</td>\n",
       "      <td>E Waste Collector</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>9623.0501</td>\n",
       "      <td>ASC/Q9601</td>\n",
       "      <td>PUC Attendant Level 2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>9623.0601</td>\n",
       "      <td>ASC/Q9603</td>\n",
       "      <td>Tyre Inflation Attendant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>9623.9900</td>\n",
       "      <td>ASC/Q9602</td>\n",
       "      <td>QCP Attendant</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Code QP_NOS Reference  \\\n",
       "0    1120.3402        MES/Q0207   \n",
       "1    1120.3500        MES/Q0201   \n",
       "2    1213.0102        ASC/Q6305   \n",
       "3    1213.0202        ASC/Q0603   \n",
       "4    1213.9900        ASC/Q0602   \n",
       "..         ...              ...   \n",
       "709  9333.9900        LSC/Q1110   \n",
       "710  9611.9900        TEL/Q2400   \n",
       "711  9623.0501        ASC/Q9601   \n",
       "712  9623.0601        ASC/Q9603   \n",
       "713  9623.9900        ASC/Q9602   \n",
       "\n",
       "                                       QP_NOS Name NSQF_Level  \n",
       "0            Account Director (Advertising Agency)          8  \n",
       "1                       Sales Director (Media Org)          8  \n",
       "2    Quality Assurance Standards In Charge-Level 5          5  \n",
       "3                             Area Service Manager          6  \n",
       "4                        Territory Service Manager          5  \n",
       "..                                             ...        ...  \n",
       "709                                Loader/Unloader          2  \n",
       "710                              E Waste Collector          3  \n",
       "711                          PUC Attendant Level 2          2  \n",
       "712                       Tyre Inflation Attendant          2  \n",
       "713                                  QCP Attendant          2  \n",
       "\n",
       "[714 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the text\n",
    "with open('/home/harikrishnan/Statathon/nco-semantic-search/data/processed/test_quali.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Regex pattern to capture the four fields\n",
    "# Logic: Look for \"Qualification Pack Details\" before the code block\n",
    "pattern = re.compile(\n",
    "    r'Qualification Pack Details:\\s*'\n",
    "    r'QP NOS Reference\\s+(?P<QP_Reference>\\S+)\\s+'\n",
    "    r'QP NOS Name\\s+(?P<QP_Name>.*?)\\s+'\n",
    "    r'NSQF Level\\s+(?P<NSQF_Level>\\d+).*?'\n",
    "    r'(?P<Code>\\d{4}\\.\\d{4})',   # Capture the code right after the details\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "# Step 3: Extract all matches\n",
    "records = []\n",
    "for match in pattern.finditer(text):\n",
    "    records.append({\n",
    "        'Code': match.group('Code').strip(),\n",
    "        'QP_NOS Reference': match.group('QP_Reference').strip(),\n",
    "        'QP_NOS Name': match.group('QP_Name').strip(),\n",
    "        'NSQF_Level': match.group('NSQF_Level').strip()\n",
    "    })\n",
    "\n",
    "# Step 4: Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Step 5: Save or display\n",
    "df.to_csv('/home/harikrishnan/Statathon/nco-semantic-search/data/processed/extracted_qp_codes.csv', index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset saved to:\n",
      "/home/harikrishnan/Statathon/nco-semantic-search/data/processed/hierarchy_with_qps.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division</th>\n",
       "      <th>Division_Description</th>\n",
       "      <th>Sub_Division</th>\n",
       "      <th>Sub_Division_Description</th>\n",
       "      <th>Group</th>\n",
       "      <th>Group_Description</th>\n",
       "      <th>Family</th>\n",
       "      <th>Family_Description</th>\n",
       "      <th>Unit_Code</th>\n",
       "      <th>Unit_Title</th>\n",
       "      <th>Unit_Description</th>\n",
       "      <th>NCO_2004</th>\n",
       "      <th>Code</th>\n",
       "      <th>QP_NOS Reference</th>\n",
       "      <th>QP_NOS Name</th>\n",
       "      <th>NSQF_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Managers</td>\n",
       "      <td>Managers plan, direct, co-ordinate and evaluat...</td>\n",
       "      <td>11 Chief Executives, Senior Officials and Legi...</td>\n",
       "      <td>Chief Executives, Senior Officials and Legisla...</td>\n",
       "      <td>111 Legislators and Senior Officials</td>\n",
       "      <td>Legislators and senior officials determine, fo...</td>\n",
       "      <td>1111 Legislators</td>\n",
       "      <td>Legislators determine, formulate, and direct p...</td>\n",
       "      <td>1111.01</td>\n",
       "      <td>Elected Official, Union Government</td>\n",
       "      <td>Elected Official, Union Government serves in v...</td>\n",
       "      <td>1111.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Managers</td>\n",
       "      <td>Managers plan, direct, co-ordinate and evaluat...</td>\n",
       "      <td>11 Chief Executives, Senior Officials and Legi...</td>\n",
       "      <td>Chief Executives, Senior Officials and Legisla...</td>\n",
       "      <td>111 Legislators and Senior Officials</td>\n",
       "      <td>Legislators and senior officials determine, fo...</td>\n",
       "      <td>1111 Legislators</td>\n",
       "      <td>Legislators determine, formulate, and direct p...</td>\n",
       "      <td>1111.02</td>\n",
       "      <td>Elected Official, State Government</td>\n",
       "      <td>Elected Official, State Government serves in v...</td>\n",
       "      <td>1112.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Managers</td>\n",
       "      <td>Managers plan, direct, co-ordinate and evaluat...</td>\n",
       "      <td>11 Chief Executives, Senior Officials and Legi...</td>\n",
       "      <td>Chief Executives, Senior Officials and Legisla...</td>\n",
       "      <td>111 Legislators and Senior Officials</td>\n",
       "      <td>Legislators and senior officials determine, fo...</td>\n",
       "      <td>1111 Legislators</td>\n",
       "      <td>Legislators determine, formulate, and direct p...</td>\n",
       "      <td>1111.03</td>\n",
       "      <td>Elected Official, Local Bodies</td>\n",
       "      <td>Elected Official, Local Bodies serves in vario...</td>\n",
       "      <td>1113.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Managers</td>\n",
       "      <td>Managers plan, direct, co-ordinate and evaluat...</td>\n",
       "      <td>11 Chief Executives, Senior Officials and Legi...</td>\n",
       "      <td>Chief Executives, Senior Officials and Legisla...</td>\n",
       "      <td>111 Legislators and Senior Officials</td>\n",
       "      <td>Legislators and senior officials determine, fo...</td>\n",
       "      <td>1111 Legislators</td>\n",
       "      <td>Legislators determine, formulate, and direct p...</td>\n",
       "      <td>1111.99</td>\n",
       "      <td>Legislators, Other</td>\n",
       "      <td>Elected Officials, Other include all other Ele...</td>\n",
       "      <td>1119.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Managers</td>\n",
       "      <td>Managers plan, direct, co-ordinate and evaluat...</td>\n",
       "      <td>11 Chief Executives, Senior Officials and Legi...</td>\n",
       "      <td>Chief Executives, Senior Officials and Legisla...</td>\n",
       "      <td>111 Legislators and Senior Officials</td>\n",
       "      <td>Legislators and senior officials determine, fo...</td>\n",
       "      <td>1112 Senior Government Officials</td>\n",
       "      <td>Senior government officials advise governments...</td>\n",
       "      <td>1112.01</td>\n",
       "      <td>Administrative Official, Union Government</td>\n",
       "      <td>Administrative Official, Union Government serv...</td>\n",
       "      <td>1121.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Division                               Division_Description  \\\n",
       "0  1 Managers  Managers plan, direct, co-ordinate and evaluat...   \n",
       "1  1 Managers  Managers plan, direct, co-ordinate and evaluat...   \n",
       "2  1 Managers  Managers plan, direct, co-ordinate and evaluat...   \n",
       "3  1 Managers  Managers plan, direct, co-ordinate and evaluat...   \n",
       "4  1 Managers  Managers plan, direct, co-ordinate and evaluat...   \n",
       "\n",
       "                                        Sub_Division  \\\n",
       "0  11 Chief Executives, Senior Officials and Legi...   \n",
       "1  11 Chief Executives, Senior Officials and Legi...   \n",
       "2  11 Chief Executives, Senior Officials and Legi...   \n",
       "3  11 Chief Executives, Senior Officials and Legi...   \n",
       "4  11 Chief Executives, Senior Officials and Legi...   \n",
       "\n",
       "                            Sub_Division_Description  \\\n",
       "0  Chief Executives, Senior Officials and Legisla...   \n",
       "1  Chief Executives, Senior Officials and Legisla...   \n",
       "2  Chief Executives, Senior Officials and Legisla...   \n",
       "3  Chief Executives, Senior Officials and Legisla...   \n",
       "4  Chief Executives, Senior Officials and Legisla...   \n",
       "\n",
       "                                  Group  \\\n",
       "0  111 Legislators and Senior Officials   \n",
       "1  111 Legislators and Senior Officials   \n",
       "2  111 Legislators and Senior Officials   \n",
       "3  111 Legislators and Senior Officials   \n",
       "4  111 Legislators and Senior Officials   \n",
       "\n",
       "                                   Group_Description  \\\n",
       "0  Legislators and senior officials determine, fo...   \n",
       "1  Legislators and senior officials determine, fo...   \n",
       "2  Legislators and senior officials determine, fo...   \n",
       "3  Legislators and senior officials determine, fo...   \n",
       "4  Legislators and senior officials determine, fo...   \n",
       "\n",
       "                             Family  \\\n",
       "0                  1111 Legislators   \n",
       "1                  1111 Legislators   \n",
       "2                  1111 Legislators   \n",
       "3                  1111 Legislators   \n",
       "4  1112 Senior Government Officials   \n",
       "\n",
       "                                  Family_Description  Unit_Code  \\\n",
       "0  Legislators determine, formulate, and direct p...    1111.01   \n",
       "1  Legislators determine, formulate, and direct p...    1111.02   \n",
       "2  Legislators determine, formulate, and direct p...    1111.03   \n",
       "3  Legislators determine, formulate, and direct p...    1111.99   \n",
       "4  Senior government officials advise governments...    1112.01   \n",
       "\n",
       "                                  Unit_Title  \\\n",
       "0         Elected Official, Union Government   \n",
       "1         Elected Official, State Government   \n",
       "2             Elected Official, Local Bodies   \n",
       "3                         Legislators, Other   \n",
       "4  Administrative Official, Union Government   \n",
       "\n",
       "                                    Unit_Description NCO_2004  Code  \\\n",
       "0  Elected Official, Union Government serves in v...  1111.10   NaN   \n",
       "1  Elected Official, State Government serves in v...  1112.10   NaN   \n",
       "2  Elected Official, Local Bodies serves in vario...  1113.10   NaN   \n",
       "3  Elected Officials, Other include all other Ele...  1119.90   NaN   \n",
       "4  Administrative Official, Union Government serv...  1121.10   NaN   \n",
       "\n",
       "  QP_NOS Reference QP_NOS Name  NSQF_Level  \n",
       "0              NaN         NaN         NaN  \n",
       "1              NaN         NaN         NaN  \n",
       "2              NaN         NaN         NaN  \n",
       "3              NaN         NaN         NaN  \n",
       "4              NaN         NaN         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "qp_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/extracted_qp_codes.csv\"\n",
    "hierarchy_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_hierarchy_with_2004.csv\"\n",
    "output_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/hierarchy_with_qps.csv\"\n",
    "\n",
    "# === Load data ===\n",
    "df_qp = pd.read_csv(qp_path)\n",
    "df_hierarchy = pd.read_csv(hierarchy_path)\n",
    "\n",
    "# === Merge so hierarchy columns come first ===\n",
    "merged_df = df_hierarchy.merge(\n",
    "    df_qp,\n",
    "    left_on=\"Unit_Code\",\n",
    "    right_on=\"Code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === Save merged data ===\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Merged dataset saved to:\\n{output_path}\")\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Division  \\\n",
      "3133      8 Plant and Machine Operators, and Assemblers   \n",
      "144                                          1 Managers   \n",
      "1231          3 Technicians and Associate Professionals   \n",
      "3000      8 Plant and Machine Operators, and Assemblers   \n",
      "1052          3 Technicians and Associate Professionals   \n",
      "2862      8 Plant and Machine Operators, and Assemblers   \n",
      "410                                     2 Professionals   \n",
      "2574                 7 Craft and Related Trades Workers   \n",
      "315                                     2 Professionals   \n",
      "2080                 7 Craft and Related Trades Workers   \n",
      "1961                 7 Craft and Related Trades Workers   \n",
      "3090      8 Plant and Machine Operators, and Assemblers   \n",
      "2679      8 Plant and Machine Operators, and Assemblers   \n",
      "1850                 7 Craft and Related Trades Workers   \n",
      "2127                 7 Craft and Related Trades Workers   \n",
      "2348                 7 Craft and Related Trades Workers   \n",
      "2332                 7 Craft and Related Trades Workers   \n",
      "3212      8 Plant and Machine Operators, and Assemblers   \n",
      "1455                  4 Clerks/Clerical Support Workers   \n",
      "1641  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "\n",
      "                                   Division_Description  \\\n",
      "3133  Plant and Machine Operators, and Assemblers op...   \n",
      "144   Managers plan, direct, co-ordinate and evaluat...   \n",
      "1231  Technicians and Associate Professionals perfor...   \n",
      "3000  Plant and Machine Operators, and Assemblers op...   \n",
      "1052  Technicians and Associate Professionals perfor...   \n",
      "2862  Plant and Machine Operators, and Assemblers op...   \n",
      "410   Professionals increase the existing stock of k...   \n",
      "2574  Craft and related trades workers apply specifi...   \n",
      "315   Professionals increase the existing stock of k...   \n",
      "2080  Craft and related trades workers apply specifi...   \n",
      "1961  Craft and related trades workers apply specifi...   \n",
      "3090  Plant and Machine Operators, and Assemblers op...   \n",
      "2679  Plant and Machine Operators, and Assemblers op...   \n",
      "1850  Craft and related trades workers apply specifi...   \n",
      "2127  Craft and related trades workers apply specifi...   \n",
      "2348  Craft and related trades workers apply specifi...   \n",
      "2332  Craft and related trades workers apply specifi...   \n",
      "3212  Plant and Machine Operators, and Assemblers op...   \n",
      "1455  Clerical support workers record, organize, sto...   \n",
      "1641  Skilled agricultural, forestry and fishery wor...   \n",
      "\n",
      "                                           Sub_Division  \\\n",
      "3133                                      82 Assemblers   \n",
      "144     13 Production and Specialized Services Managers   \n",
      "1231  33 Business and Administration Associate Profe...   \n",
      "3000          81 Stationary Plant and Machine Operators   \n",
      "1052  31 Science and Engineering Associate Professio...   \n",
      "2862          81 Stationary Plant and Machine Operators   \n",
      "410            21 Science and Engineering Professionals   \n",
      "2574  75 Food Processing, Woodworking, Garment and O...   \n",
      "315            21 Science and Engineering Professionals   \n",
      "2080                 73 Handicraft and Printing Workers   \n",
      "1961     72 Metal, Machinery and Related Trades Workers   \n",
      "3090          81 Stationary Plant and Machine Operators   \n",
      "2679          81 Stationary Plant and Machine Operators   \n",
      "1850     72 Metal, Machinery and Related Trades Workers   \n",
      "2127                 73 Handicraft and Printing Workers   \n",
      "2348  75 Food Processing, Woodworking, Garment and O...   \n",
      "2332       74 Electrical and Electronics Trades Workers   \n",
      "3212              83 Drivers and Mobile Plant Operators   \n",
      "1455                  44 Other Clerical Support Workers   \n",
      "1641    61 Market Oriented Skilled Agricultural Workers   \n",
      "\n",
      "                               Sub_Division_Description  \\\n",
      "3133  Assemblers assemble prefabricated parts or com...   \n",
      "144   Production and Specialized Services managers p...   \n",
      "1231  Business and Administration Associate professi...   \n",
      "3000  Stationary Plant and Machine Operators operate...   \n",
      "1052  Science and Engineering Associate Professional...   \n",
      "2862  Stationary Plant and Machine Operators operate...   \n",
      "410   Physical, Mathematical and Engineering Science...   \n",
      "2574  Food processing, wood working, garment and oth...   \n",
      "315   Physical, Mathematical and Engineering Science...   \n",
      "2080  Handicraft and Printing Workers make and repai...   \n",
      "1961  Metal, machinery and related trades workers ca...   \n",
      "3090  Stationary Plant and Machine Operators operate...   \n",
      "2679  Stationary Plant and Machine Operators operate...   \n",
      "1850  Metal, machinery and related trades workers ca...   \n",
      "2127  Handicraft and Printing Workers make and repai...   \n",
      "2348  Food processing, wood working, garment and oth...   \n",
      "2332  Electrical and electronics trades workers inst...   \n",
      "3212  Drivers and Mobile Plant Operators drive and t...   \n",
      "1455  Other clerks perform clerical duties in newspa...   \n",
      "1641  Market-oriented skilled agricultural workers p...   \n",
      "\n",
      "                                                  Group  \\\n",
      "3133                                     821 Assemblers   \n",
      "144   132 Manufacturing, Mining, Construction and Di...   \n",
      "1231        332 Sales and Purchasing Agents and Brokers   \n",
      "3000    816 Food and Related Products Machine Operators   \n",
      "1052                    313 Process Control Technicians   \n",
      "2862  815 Textile, Fur and Leather Products Machine ...   \n",
      "410   216 Architects, Planners, Surveyors and Designers   \n",
      "2574                754 Other Craft and Related Workers   \n",
      "315   214 Engineering Professionals (excluding Elect...   \n",
      "2080                             731 Handicraft Workers   \n",
      "1961              723 Machinery Mechanics and Repairers   \n",
      "3090   818 Other Stationary Plant and Machine Operators   \n",
      "2679  812 Metal Processing and Finishing Plant Opera...   \n",
      "1850  721 Sheet and Structural Metal Workers, Moulde...   \n",
      "2127                             731 Handicraft Workers   \n",
      "2348      751 Food Processing and Related Trade Workers   \n",
      "2332  742 Electronics and Telecommunication Installe...   \n",
      "3212                         834 Mobile Plant Operators   \n",
      "1455                 441 Other Clerical Support Workers   \n",
      "1641              611 Market Gardeners and Crop Growers   \n",
      "\n",
      "                                      Group_Description  \\\n",
      "3133  Assemblers assemble prefabricated parts or com...   \n",
      "144   Manufacturing, Mining, Construction, and Distr...   \n",
      "1231  Sales and purchasing agents and brokers repres...   \n",
      "3000  Food and Related Machine Operators set, operat...   \n",
      "1052  Process Control Technicians operate and monito...   \n",
      "2862  Textile, Fur and Leather Products Machine Oper...   \n",
      "410   Architects, Planners, Surveyors and designers ...   \n",
      "2574  Other craft and related workers work under the...   \n",
      "315   Engineering Professionals (excluding Electro-t...   \n",
      "2080  Handicraft workers combine artistic and manual...   \n",
      "1961  Machinery mechanics and repairers fit, install...   \n",
      "3090  This group includes Stationary Plant and Machi...   \n",
      "2679  Metal Processing and Finishing Plant Operators...   \n",
      "1850  Sheet and structural metal workers, moulders a...   \n",
      "2127  Handicraft workers combine artistic and manual...   \n",
      "2348  Food processing and related trades workers sla...   \n",
      "2332  Electronics and telecommunications installers ...   \n",
      "3212  Mobile Plant Operators drive, tend, operate an...   \n",
      "1455  Other Clerks perform clerical duties in newspa...   \n",
      "1641  Market gardeners and crop growers plan, organi...   \n",
      "\n",
      "                                                 Family  \\\n",
      "3133  8212 Electrical and Electronic Equipment Assem...   \n",
      "144      1324 Supply, Distribution and Related Managers   \n",
      "1231              3322 Commercial Sales Representatives   \n",
      "3000   8160 Food and Related Products Machine Operators   \n",
      "1052         3133 Chemical Processing Plant Controllers   \n",
      "2862                      8153 Sewing Machine Operators   \n",
      "410                    2165 Cartographers and Surveyors   \n",
      "2574  7543 Product Graders and Testers (Excluding Fo...   \n",
      "315                           2144 Mechanical Engineers   \n",
      "2080         7313 Jewellery and Precision Metal Workers   \n",
      "1961         7231 Motor Vehicle Mechanics and Repairers   \n",
      "3090             8182 Steam-Engine and Boiler Operators   \n",
      "2679  8122 Metal Finishing, Plating and Coating Mach...   \n",
      "1850                     7212 Welders and Flame Cutters   \n",
      "2127  7315 Glass Makers, Cutters, Grinders and Finis...   \n",
      "2348  7511 Butchers, Fishmongers and Related Food Pr...   \n",
      "2332  7422 Information and Communications Technology...   \n",
      "3212      8342 Earth Moving and Related Plant Operators   \n",
      "1455                                4411 Library Clerks   \n",
      "1641              6111 Field Crop and Vegetable Growers   \n",
      "\n",
      "                                     Family_Description  Unit_Code  \\\n",
      "3133  Electrical and Electronic Equipment Assemblers...  8212.1602   \n",
      "144   Supply, Distribution and Related Managers plan...  1324.1201   \n",
      "1231  Commercial sales representatives represent com...  3322.2101   \n",
      "3000  Food and Related Machine Operators set, operat...  8160.6500   \n",
      "1052  Chemical processing plant controllers operate ...  3133.0800   \n",
      "2862  Sewing Machine Operators operate and monitor s...  8153.0601   \n",
      "410   Cartographers and Surveyors determine the exac...  2165.9900   \n",
      "2574  Product graders and testers (except foods and ...  7543.3004   \n",
      "315   Mechanical Engineers conduct research; advise ...  2144.0301   \n",
      "2080  Jewellery and precious metal workers design, f...  7313.1400   \n",
      "1961  Motor vehicle mechanics and repairers fit, ins...  7231.0102   \n",
      "3090  Steam Engine and Boiler Operators maintain and...  8182.9900   \n",
      "2679  Metal Finishing, Plating and Coating Machine O...  8122.8100   \n",
      "1850  VOLUM Divison 7 Welders and flame cutters weld...  7212.0302   \n",
      "2127  Glass makers, cutters, grinders and finishers ...  7315.1600   \n",
      "2348  Butchers, fishmongers and related food prepare...  7511.0100   \n",
      "2332  Information and communications technology (ICT...  7422.1202   \n",
      "3212  Earth Moving and Related Plant Operators opera...  8342.0101   \n",
      "1455  Library Clerks issue and receive library mater...  4411.0300   \n",
      "1641  Field crop and vegetable growers plan, organiz...  6111.0901   \n",
      "\n",
      "                                             Unit_Title  \\\n",
      "3133                     Through Hole Assembly Operator   \n",
      "144                                Warehouse Supervisor   \n",
      "1231      Regional Retail Finance and Insurance Manager   \n",
      "3000                                     Coffee Roaster   \n",
      "1052                                Hot-Cell Technician   \n",
      "2862      Embroidery Machine Operator (Zig Zag Machine)   \n",
      "410                  Cartographers and Surveyors, Other   \n",
      "2574  Quality Control Inspector-Statistical process ...   \n",
      "315                                  Equipment Designer   \n",
      "2080                                 Gem Polisher, Hand   \n",
      "1961                                      AC Specialist   \n",
      "3090                  Ships’ Engine Room Ratings, Other   \n",
      "2679                Levelling Machine Attendant (Match)   \n",
      "1850                                 Welding Technician   \n",
      "2127                              Wheel Grinder (Glass)   \n",
      "2348                                            Butcher   \n",
      "2332   DTH Set-top Box Installer and Service Technician   \n",
      "3212                                 Bulldozer Operator   \n",
      "1455                   Preservation Assistant, Archives   \n",
      "1641                                      Banana Farmer   \n",
      "\n",
      "                                       Unit_Description NCO_2004  \\\n",
      "3133  Through hole assembly operator inserts electro...      NaN   \n",
      "144   Warehouse Supervisor in the Logistics industry...      NaN   \n",
      "1231  Regional Retail Finance and Insurance Manager ...      NaN   \n",
      "3000  Coffee Roaster; Coffee-roasting Machine Operat...  8277.50   \n",
      "1052  Hot-Cell Technician operates remote- controlle...  8159.52   \n",
      "2862  Embroidery Machine Operator is responsible for...      NaN   \n",
      "410   Cartographers and Surveyors, Other Cartographe...  2148.90   \n",
      "2574  Quality Control Inspector-Statistical process ...      NaN   \n",
      "315   Equipment Designer is also known as Tool Desig...      NaN   \n",
      "2080  Gem Polisher, Hand polishes gems according to ...  7313.60   \n",
      "1961  AC Specialist is responsible for installing, s...      NaN   \n",
      "3090  Ships' Engine Room Ratings, Other include all ...  8162.90   \n",
      "2679  Levelling Machine Attendant (Match); Leveller ...  8223.81   \n",
      "1850  Welding Technician, this role is similar for a...      NaN   \n",
      "2127  Wheel Grinder, Glass grinds and finishes rough...  7322.38   \n",
      "2348  Butcher slaughters animals, trims carcasses an...  7411.10   \n",
      "2332  DTH Set-Top Box Installer and Service technici...      NaN   \n",
      "3212  Bulldozer Operator operates bulldozer (caterpi...  8332.10   \n",
      "1455  Preservation Assistant, Archives; maintains ar...  4141.30   \n",
      "1641  Banana Farmer cultivates banana as per the pac...      NaN   \n",
      "\n",
      "     QP_NOS Reference                                        QP_NOS Name  \\\n",
      "3133        ELE/Q5101                     Through Hole Assembly Operator   \n",
      "144               NaN                                                NaN   \n",
      "1231              NaN                                                NaN   \n",
      "3000              NaN                                                NaN   \n",
      "1052              NaN                                                NaN   \n",
      "2862              NaN                                                NaN   \n",
      "410               NaN                                                NaN   \n",
      "2574        RSC/Q0416  Quality Control Inspector-Statistical process ...   \n",
      "315               NaN                                                NaN   \n",
      "2080        G&J/Q2505                                   Diamond Assorter   \n",
      "1961        ASC/Q1409                                      AC Specialist   \n",
      "3090              NaN                                                NaN   \n",
      "2679              NaN                                                NaN   \n",
      "1850        ASC/Q3102                         Welding Technician Level 3   \n",
      "2127              NaN                                                NaN   \n",
      "2348              NaN                                                NaN   \n",
      "2332        ELE/Q8101   DTH Set-top Box Installer and Service Technician   \n",
      "3212              NaN                                                NaN   \n",
      "1455              NaN                                                NaN   \n",
      "1641        AGR/Q0201                                      Banana Farmer   \n",
      "\n",
      "      NSQF_Level  \n",
      "3133         3.0  \n",
      "144          NaN  \n",
      "1231         NaN  \n",
      "3000         NaN  \n",
      "1052         NaN  \n",
      "2862         NaN  \n",
      "410          NaN  \n",
      "2574         NaN  \n",
      "315          NaN  \n",
      "2080         4.0  \n",
      "1961         4.0  \n",
      "3090         NaN  \n",
      "2679         NaN  \n",
      "1850         3.0  \n",
      "2127         NaN  \n",
      "2348         NaN  \n",
      "2332         4.0  \n",
      "3212         NaN  \n",
      "1455         NaN  \n",
      "1641         4.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "merged_path = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/hierarchy_with_qps_clean.csv\"\n",
    "\n",
    "\n",
    "# Load merged data\n",
    "df = pd.read_csv(merged_path)\n",
    "\n",
    "# Select only the required columns\n",
    "\n",
    "\n",
    "# Show 20 random rows\n",
    "print(df[:].sample(20, random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "np.save('embeddings/nco_embeddings.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /home/harikrishnan/venvs/statathon_env/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m627.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fully cleaned text saved to: ../data/processed/cleaned_text2.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_and_remove_annotations(input_path: str, output_path: str):\n",
    "    remove_next_line = False\n",
    "    cleaned_lines = []\n",
    "\n",
    "    # === Patterns ===\n",
    "    # Matches lines like: \"7 E II A\", \"25 E II A\"\n",
    "    annotation_line_pattern = re.compile(r'^\\s*\\d+\\s+(E\\s+)?II\\s+A\\s*$', re.IGNORECASE)\n",
    "\n",
    "    # Matches inline: \"47 E II A\", \"63 II A\", etc.\n",
    "    embedded_annotation_pattern = re.compile(r'\\b\\d+\\s+(E\\s+)?II\\s*A\\b', re.IGNORECASE)\n",
    "\n",
    "    # === Read all lines ===\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "\n",
    "        # Skip the line after \"ISCO...\" lines\n",
    "        if remove_next_line:\n",
    "            remove_next_line = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Skip lines with \"ISCO 08 Unit Group Details:\"\n",
    "        if \"ISCO 08 Unit Group Details:\" in line:\n",
    "            remove_next_line = True\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Skip full annotation lines like \"63 II A\", \"7 E II A\"\n",
    "        if annotation_line_pattern.match(line.strip()):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Remove embedded annotations inside the line\n",
    "        cleaned_line = embedded_annotation_pattern.sub('', line)\n",
    "\n",
    "        # Clean up double/multiple spaces if it's not a blank line\n",
    "        if cleaned_line.strip():\n",
    "            cleaned_line = re.sub(r'\\s{2,}', ' ', cleaned_line).strip()\n",
    "            cleaned_lines.append(cleaned_line + '\\n')\n",
    "        else:\n",
    "            # Preserve blank lines\n",
    "            cleaned_lines.append('\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # === Write cleaned output ===\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"✅ Fully cleaned text saved to: {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file_path = \"../data/processed/test.txt\"\n",
    "    output_file_path = \"../data/processed/cleaned_text2.txt\"\n",
    "    clean_and_remove_annotations(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1803 occupation records.\n",
      "        Code                Title  \\\n",
      "0  6111.0100  Cultivator, General   \n",
      "1  6111.0101         Paddy Farmer   \n",
      "2  6111.0200     Cultivator, Crop   \n",
      "3  6111.0201     Wheat Cultivator   \n",
      "4  6111.0301     Maize Cultivator   \n",
      "\n",
      "                                         Description  \n",
      "0  Cultivators, General; Farmer, General; grows c...  \n",
      "1  Paddy Farmer cultivates paddy as per the packa...  \n",
      "2  Cultivator, Crop; Farmer, Crop grows field cro...  \n",
      "3  Wheat Cultivator cultivates wheat as per the p...  \n",
      "4  Maize Cultivator undertakes the cultivation of...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_isco_text(raw_text):\n",
    "    \"\"\"\n",
    "    Parses raw ISCO occupational classification text into a structured DataFrame.\n",
    "    Assumes each occupation entry starts with a code (e.g., '1111.0100') followed by a title,\n",
    "    then a detailed multi-line description until the next code or end of text.\n",
    "    \"\"\"\n",
    "    # Pattern to detect occupation code and title lines\n",
    "    # Example code formats: '1111.0100', '1120', '11', etc.\n",
    "    # This pattern captures codes like \"1111.0100\" or \"1111\"\n",
    "    code_title_pattern = re.compile(r'^(\\d{2,7}(?:\\.\\d{4})?)\\s+(.+)$', re.MULTILINE)\n",
    "    \n",
    "    # Find all matches for codes and titles\n",
    "    matches = list(code_title_pattern.finditer(raw_text))\n",
    "    \n",
    "    records = []\n",
    "    for i, match in enumerate(matches):\n",
    "        code = match.group(1).strip()\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Start of description: from this match end to next match start or end of text\n",
    "        start_desc = match.end()\n",
    "        end_desc = matches[i+1].start() if i+1 < len(matches) else len(raw_text)\n",
    "        description = raw_text[start_desc:end_desc].strip()\n",
    "        \n",
    "        # Clean description: remove excessive newlines and spaces\n",
    "        description = re.sub(r'\\n+', ' ', description)\n",
    "        description = re.sub(r'\\s+', ' ', description)\n",
    "        \n",
    "        records.append({\n",
    "            \"Code\": code,\n",
    "            \"Title\": title,\n",
    "            \"Description\": description\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the raw text from a file, e.g. \"isco_raw.txt\"\n",
    "    with open(\"../data/processed/cleaned_text2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    df_isco = parse_isco_text(raw_text)\n",
    "    df_isco.to_csv(\"../data/processed/isco_dataset2.csv\", index=False)\n",
    "    print(f\"Extracted {len(df_isco)} occupation records.\")\n",
    "    print(df_isco.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1747 records with detailed hierarchy.\n",
      "                                            Division  \\\n",
      "0  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "1  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "2  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "3  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "4  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "5  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "6  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "7  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "8  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "9  6 Skilled Agricultural, Forestry and Fishery W...   \n",
      "\n",
      "                                Division_Description  \\\n",
      "0  Skilled agricultural, forestry and fishery wor...   \n",
      "1  Skilled agricultural, forestry and fishery wor...   \n",
      "2  Skilled agricultural, forestry and fishery wor...   \n",
      "3  Skilled agricultural, forestry and fishery wor...   \n",
      "4  Skilled agricultural, forestry and fishery wor...   \n",
      "5  Skilled agricultural, forestry and fishery wor...   \n",
      "6  Skilled agricultural, forestry and fishery wor...   \n",
      "7  Skilled agricultural, forestry and fishery wor...   \n",
      "8  Skilled agricultural, forestry and fishery wor...   \n",
      "9  Skilled agricultural, forestry and fishery wor...   \n",
      "\n",
      "                                      Sub_Division  \\\n",
      "0  61 Market Oriented Skilled Agricultural Workers   \n",
      "1  61 Market Oriented Skilled Agricultural Workers   \n",
      "2  61 Market Oriented Skilled Agricultural Workers   \n",
      "3  61 Market Oriented Skilled Agricultural Workers   \n",
      "4  61 Market Oriented Skilled Agricultural Workers   \n",
      "5  61 Market Oriented Skilled Agricultural Workers   \n",
      "6  61 Market Oriented Skilled Agricultural Workers   \n",
      "7  61 Market Oriented Skilled Agricultural Workers   \n",
      "8  61 Market Oriented Skilled Agricultural Workers   \n",
      "9  61 Market Oriented Skilled Agricultural Workers   \n",
      "\n",
      "                            Sub_Division_Description  \\\n",
      "0  Market-oriented skilled agricultural workers p...   \n",
      "1  Market-oriented skilled agricultural workers p...   \n",
      "2  Market-oriented skilled agricultural workers p...   \n",
      "3  Market-oriented skilled agricultural workers p...   \n",
      "4  Market-oriented skilled agricultural workers p...   \n",
      "5  Market-oriented skilled agricultural workers p...   \n",
      "6  Market-oriented skilled agricultural workers p...   \n",
      "7  Market-oriented skilled agricultural workers p...   \n",
      "8  Market-oriented skilled agricultural workers p...   \n",
      "9  Market-oriented skilled agricultural workers p...   \n",
      "\n",
      "                                   Group  \\\n",
      "0  611 Market Gardeners and Crop Growers   \n",
      "1  611 Market Gardeners and Crop Growers   \n",
      "2  611 Market Gardeners and Crop Growers   \n",
      "3  611 Market Gardeners and Crop Growers   \n",
      "4  611 Market Gardeners and Crop Growers   \n",
      "5  611 Market Gardeners and Crop Growers   \n",
      "6  611 Market Gardeners and Crop Growers   \n",
      "7  611 Market Gardeners and Crop Growers   \n",
      "8  611 Market Gardeners and Crop Growers   \n",
      "9  611 Market Gardeners and Crop Growers   \n",
      "\n",
      "                                   Group_Description  \\\n",
      "0  Market gardeners and crop growers plan, organi...   \n",
      "1  Market gardeners and crop growers plan, organi...   \n",
      "2  Market gardeners and crop growers plan, organi...   \n",
      "3  Market gardeners and crop growers plan, organi...   \n",
      "4  Market gardeners and crop growers plan, organi...   \n",
      "5  Market gardeners and crop growers plan, organi...   \n",
      "6  Market gardeners and crop growers plan, organi...   \n",
      "7  Market gardeners and crop growers plan, organi...   \n",
      "8  Market gardeners and crop growers plan, organi...   \n",
      "9  Market gardeners and crop growers plan, organi...   \n",
      "\n",
      "                                  Family  \\\n",
      "0  6111 Field Crop and Vegetable Growers   \n",
      "1  6111 Field Crop and Vegetable Growers   \n",
      "2  6111 Field Crop and Vegetable Growers   \n",
      "3  6111 Field Crop and Vegetable Growers   \n",
      "4  6111 Field Crop and Vegetable Growers   \n",
      "5  6111 Field Crop and Vegetable Growers   \n",
      "6  6111 Field Crop and Vegetable Growers   \n",
      "7  6111 Field Crop and Vegetable Growers   \n",
      "8  6111 Field Crop and Vegetable Growers   \n",
      "9  6111 Field Crop and Vegetable Growers   \n",
      "\n",
      "                                  Family_Description  Unit_Code  \\\n",
      "0  Field crop and vegetable growers plan, organiz...  6111.0100   \n",
      "1  Field crop and vegetable growers plan, organiz...  6111.0101   \n",
      "2  Field crop and vegetable growers plan, organiz...  6111.0200   \n",
      "3  Field crop and vegetable growers plan, organiz...  6111.0201   \n",
      "4  Field crop and vegetable growers plan, organiz...  6111.0301   \n",
      "5  Field crop and vegetable growers plan, organiz...  6111.0401   \n",
      "6  Field crop and vegetable growers plan, organiz...  6111.0501   \n",
      "7  Field crop and vegetable growers plan, organiz...  6111.0601   \n",
      "8  Field crop and vegetable growers plan, organiz...  6111.0701   \n",
      "9  Field crop and vegetable growers plan, organiz...  6111.0800   \n",
      "\n",
      "             Unit_Title                                   Unit_Description  \n",
      "0   Cultivator, General  Cultivators, General; Farmer, General; grows c...  \n",
      "1          Paddy Farmer  Paddy Farmer cultivates paddy as per the packa...  \n",
      "2      Cultivator, Crop  Cultivator, Crop; Farmer, Crop grows field cro...  \n",
      "3      Wheat Cultivator  Wheat Cultivator cultivates wheat as per the p...  \n",
      "4      Maize Cultivator  Maize Cultivator undertakes the cultivation of...  \n",
      "5     Pulses Cultivator  Pulses Cultivator undertakes the cultivation o...  \n",
      "6  Soya bean Cultivator  Soya bean Cultivator undertakes the cultivatio...  \n",
      "7     Cotton Cultivator  Cotton Cultivator undertakes the cultivation o...  \n",
      "8  Sugarcane Cultivator  Sugarcane Cultivator undertakes the cultivatio...  \n",
      "9     Cultivator, Fruit  Cultivator, Fruit: Farmer, Fruit grows varieti...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_full_hierarchy(text):\n",
    "    # Regex patterns for each level header\n",
    "    division_pat = re.compile(r\"^Division\\s+(\\d+)\\s+(.*)$\", re.IGNORECASE)\n",
    "    sub_div_pat = re.compile(r\"^Sub\\s*Division\\s+(\\d+)\\s+(.*)$\", re.IGNORECASE)\n",
    "    group_pat = re.compile(r\"^Group\\s+(\\d+)\\s+(.*)$\", re.IGNORECASE)\n",
    "    family_pat = re.compile(r\"^Family\\s+(\\d+)\\s+(.*)$\", re.IGNORECASE)\n",
    "    # Matches Unit lines: e.g., 1111.0200 Sewing Machine Operators\n",
    "    unit_pat = re.compile(r\"^(\\d{4,7}(?:\\.\\d+)?)\\s+(.*)$\")\n",
    "\n",
    "    # Store parsed records here\n",
    "    records = []\n",
    "\n",
    "    # Current state holders\n",
    "    state = {\n",
    "        \"Division\": None,\n",
    "        \"Division_Description\": [],\n",
    "        \"Sub_Division\": None,\n",
    "        \"Sub_Division_Description\": [],\n",
    "        \"Group\": None,\n",
    "        \"Group_Description\": [],\n",
    "        \"Family\": None,\n",
    "        \"Family_Description\": [],\n",
    "        \"Unit_Code\": None,\n",
    "        \"Unit_Title\": None,\n",
    "        \"Unit_Description\": []\n",
    "    }\n",
    "\n",
    "    def flush_unit():\n",
    "        \"\"\"Save the current unit record with all hierarchy.\"\"\"\n",
    "        if state[\"Unit_Code\"] is not None:\n",
    "            rec = {\n",
    "                \"Division\": state[\"Division\"],\n",
    "                \"Division_Description\": \" \".join(state[\"Division_Description\"]).strip(),\n",
    "                \"Sub_Division\": state[\"Sub_Division\"],\n",
    "                \"Sub_Division_Description\": \" \".join(state[\"Sub_Division_Description\"]).strip(),\n",
    "                \"Group\": state[\"Group\"],\n",
    "                \"Group_Description\": \" \".join(state[\"Group_Description\"]).strip(),\n",
    "                \"Family\": state[\"Family\"],\n",
    "                \"Family_Description\": \" \".join(state[\"Family_Description\"]).strip(),\n",
    "                \"Unit_Code\": state[\"Unit_Code\"],\n",
    "                \"Unit_Title\": state[\"Unit_Title\"],\n",
    "                \"Unit_Description\": \" \".join(state[\"Unit_Description\"]).strip()\n",
    "            }\n",
    "            records.append(rec)\n",
    "            # Reset unit info after flush\n",
    "            state[\"Unit_Code\"] = None\n",
    "            state[\"Unit_Title\"] = None\n",
    "            state[\"Unit_Description\"] = []\n",
    "\n",
    "    def reset_lower_levels(level):\n",
    "        \"\"\"Reset all lower levels descriptions and identifiers after a new higher-level is found.\"\"\"\n",
    "        levels = [\"Division\", \"Sub_Division\", \"Group\", \"Family\", \"Unit_Code\"]\n",
    "        level_indices = {lvl: idx for idx, lvl in enumerate(levels)}\n",
    "        idx = level_indices[level]\n",
    "        for lvl in levels[idx+1:]:\n",
    "            state[lvl if not lvl.endswith(\"_Code\") else \"Unit_Code\"] = None\n",
    "            desc_key = lvl.replace(\"Code\", \"Description\")\n",
    "            if desc_key in state:\n",
    "                state[desc_key] = []\n",
    "\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    current_level = None  # Tracks current description target\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Match each header, starting with the highest level:\n",
    "        div_m = division_pat.match(line)\n",
    "        if div_m:\n",
    "            flush_unit()\n",
    "\n",
    "            reset_lower_levels(\"Division\")\n",
    "\n",
    "            state[\"Division\"] = f\"{div_m.group(1)} {div_m.group(2).strip()}\"\n",
    "            state[\"Division_Description\"] = []\n",
    "            current_level = \"Division_Description\"\n",
    "            continue\n",
    "\n",
    "        sub_div_m = sub_div_pat.match(line)\n",
    "        if sub_div_m:\n",
    "            flush_unit()\n",
    "\n",
    "            reset_lower_levels(\"Sub_Division\")\n",
    "\n",
    "            state[\"Sub_Division\"] = f\"{sub_div_m.group(1)} {sub_div_m.group(2).strip()}\"\n",
    "            state[\"Sub_Division_Description\"] = []\n",
    "            current_level = \"Sub_Division_Description\"\n",
    "            continue\n",
    "\n",
    "        group_m = group_pat.match(line)\n",
    "        if group_m:\n",
    "            flush_unit()\n",
    "\n",
    "            reset_lower_levels(\"Group\")\n",
    "\n",
    "            state[\"Group\"] = f\"{group_m.group(1)} {group_m.group(2).strip()}\"\n",
    "            state[\"Group_Description\"] = []\n",
    "            current_level = \"Group_Description\"\n",
    "            continue\n",
    "\n",
    "        family_m = family_pat.match(line)\n",
    "        if family_m:\n",
    "            flush_unit()\n",
    "\n",
    "            reset_lower_levels(\"Family\")\n",
    "\n",
    "            state[\"Family\"] = f\"{family_m.group(1)} {family_m.group(2).strip()}\"\n",
    "            state[\"Family_Description\"] = []\n",
    "            current_level = \"Family_Description\"\n",
    "            continue\n",
    "\n",
    "        unit_m = unit_pat.match(line)\n",
    "        if unit_m:\n",
    "            flush_unit()\n",
    "\n",
    "            reset_lower_levels(\"Unit_Code\")\n",
    "\n",
    "            state[\"Unit_Code\"] = unit_m.group(1).strip()\n",
    "            state[\"Unit_Title\"] = unit_m.group(2).strip()\n",
    "            state[\"Unit_Description\"] = []\n",
    "            current_level = \"Unit_Description\"\n",
    "            continue\n",
    "\n",
    "        # If line doesn't match headers, accumulate it in the current description\n",
    "        if current_level:\n",
    "            state[current_level].append(line)\n",
    "        else:\n",
    "            # If no current level, could log or skip\n",
    "            pass\n",
    "\n",
    "    # Flush the last unit at end of file\n",
    "    flush_unit()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # Clean whitespace in all string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace this path with your file path\n",
    "    input_file = \"../data/processed/cleaned_text2.txt\"\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    df_hierarchy = parse_full_hierarchy(raw_text)\n",
    "    output_csv = \"../data/processed/nco_full_hierarchy2.csv\"\n",
    "    df_hierarchy.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Extracted {len(df_hierarchy)} records with detailed hierarchy.\")\n",
    "    print(df_hierarchy.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files combined and saved to:\n",
      "/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_merged_hierarchy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "file1 = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_full_hierarchy.csv\"\n",
    "file2 = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_full_hierarchy2.csv\"\n",
    "output_file = \"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_merged_hierarchy.csv\"\n",
    "\n",
    "# === Load both CSVs ===\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# === Concatenate vertically ===\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# === Optional: drop duplicates (if needed) ===\n",
    "# merged_df.drop_duplicates(subset=\"Unit_Code\", keep='first', inplace=True)\n",
    "\n",
    "# === Save merged dataset ===\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Files combined and saved to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_hierarchy_with_2004.csv\")\n",
    "\n",
    "# Take the first 200 rows\n",
    "df.head(200).to_csv(\"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/sample_heirarchy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: NCO 2004 codes added where matched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "df_idx = pd.read_csv('../data/processed/nco_vol1_alp_idx.csv', header=0)\n",
    "df_idx.columns = ['Occupational_Title', 'NCO_2015', 'NCO_2004']\n",
    "\n",
    "df_hier = pd.read_csv('../data/processed/nco_merged_hierarchy.csv')\n",
    "\n",
    "# Normalize key columns (remove commas, strip)\n",
    "df_idx['NCO_2015'] = df_idx['NCO_2015'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "df_idx['NCO_2004'] = df_idx['NCO_2004'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "df_hier['Unit_Code'] = df_hier['Unit_Code'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "\n",
    "# Create lookup dictionary\n",
    "code_to_2004 = dict(zip(df_idx['NCO_2015'], df_idx['NCO_2004']))\n",
    "\n",
    "# Test first 5 unit codes\n",
    "print(\"🔍 Checking first 5 Unit_Code values:\\n\")\n",
    "\n",
    "for unit_code in df_hier['Unit_Code'].head(5):\n",
    "    match = code_to_2004.get(unit_code)\n",
    "    if match:\n",
    "        print(f\"✅ Unit_Code: {unit_code} → NCO_2004: {match}\")\n",
    "    else:\n",
    "        print(f\"❌ Unit_Code: {unit_code} → NOT FOUND in index file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NCO 2004 values added to hierarchy file based on matching Unit_Code with NCO 2015.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load index file (title to NCO codes) ===\n",
    "df_idx = pd.read_csv('../data/processed/nco_vol1_alp_idx.csv', header=0)\n",
    "df_idx.columns = ['Occupational_Title', 'NCO_2015', 'NCO_2004']\n",
    "\n",
    "# Clean index NCO_2015 and NCO_2004\n",
    "df_idx['NCO_2015'] = df_idx['NCO_2015'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "df_idx['NCO_2004'] = df_idx['NCO_2004'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "\n",
    "# === Load hierarchy file ===\n",
    "df_hier = pd.read_csv('../data/processed/nco_merged_hierarchy.csv')\n",
    "df_hier['Unit_Code'] = df_hier['Unit_Code'].astype(str).str.replace(',', '', regex=False).str.strip()\n",
    "\n",
    "# === Create a lookup dict from the index file ===\n",
    "code_to_2004 = dict(zip(df_idx['NCO_2015'], df_idx['NCO_2004']))\n",
    "\n",
    "# === Map NCO_2004 for each Unit_Code in the hierarchy file ===\n",
    "df_hier['NCO_2004'] = df_hier['Unit_Code'].map(code_to_2004)\n",
    "\n",
    "# === Save final enriched hierarchy ===\n",
    "df_hier.to_csv('../data/processed/nco_hierarchy_with_2004.csv', index=False)\n",
    "\n",
    "print(\"✅ NCO 2004 values added to hierarchy file based on matching Unit_Code with NCO 2015.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number of rows for Division '1 Managers': 118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"/home/harikrishnan/Statathon/nco-semantic-search/data/processed/nco_hierarchy_with_2004.csv\")\n",
    "\n",
    "# Split 'Division' into numeric code and title\n",
    "df[['Division_Code', 'Division_Title']] = df['Division'].str.extract(r'(\\d+)\\s+(.*)')\n",
    "\n",
    "# Normalize\n",
    "df['Division_Code'] = df['Division_Code'].astype(str).str.strip()\n",
    "df['Division_Title'] = df['Division_Title'].str.strip().str.lower()\n",
    "\n",
    "# Count rows where Division is '1 Managers'\n",
    "count = df[(df['Division_Code'] == '4') ].shape[0]\n",
    "\n",
    "print(f\"✅ Number of rows for Division '1 Managers': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files + 3 random CSV rows merged into all_project_code.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Folders to scan\n",
    "folders_to_scan = [\n",
    "    \"app\",\n",
    "    \"scripts\",\n",
    "    \"scripts_bc\",\n",
    "    \"backup_14_23_aug\"\n",
    "]\n",
    "\n",
    "# File extensions to include\n",
    "extensions = [\".py\", \".json\", \".yaml\"]\n",
    "\n",
    "output_file = \"all_project_code.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    # Merge scripts, JSON, YAML\n",
    "    for folder in folders_to_scan:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in extensions):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    out_f.write(f\"\\n\\n# ===== FILE: {file_path} =====\\n\\n\")\n",
    "                    try:\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            out_f.write(f.read())\n",
    "                    except Exception as e:\n",
    "                        out_f.write(f\"# ERROR READING FILE: {e}\\n\")\n",
    "    \n",
    "    # Append 3 random rows from nco_cleaned.csv\n",
    "    csv_path = \"data/processed/nco_cleaned.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sample_rows = df.sample(n=3, random_state=42)\n",
    "        out_f.write(\"\\n\\n# ===== 3 RANDOM ROWS FROM nco_cleaned.csv =====\\n\\n\")\n",
    "        out_f.write(sample_rows.to_csv(index=False))\n",
    "    else:\n",
    "        out_f.write(f\"\\n\\n# CSV file {csv_path} not found.\\n\")\n",
    "\n",
    "print(f\"All files + 3 random CSV rows merged into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (statthon_env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
